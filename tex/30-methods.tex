\chapter{Методы интерпретации}

В этой главе мы кратко представим существующие подходы интерпретации моделей машинного обучения.

Следует представить некоторую категоризацию имеющихся методов. Популярным подходом является разделение методов на три широкие группы: локальные, глобальные объяснения и объяснения отдельных классов. \textbf{Локальные объяснения} предназначены для ретроактивного объяснения результатов только для одного заданного наблюдения. \textbf{Глобальные объяснения} пытаются суммаризовать информацию о всей модели целиком, но обычно по отношению только к одному выбранному аспекту. \textbf{Классовые объяснения} также пытаются объяснить всю модель, но только по отношению работы с одним конкретным классом.

Также в \cite{madsenPosthocInterpretabilityNeural2021} дополнительно проводится различие между внутренними (intrinsic) и post-hoc методами. Внутренние методы в своей работы полагаются на устройство рассматриваемой модели. Зачастую это относится к моделям, которые в силу свое простой структуры изначально интерпретируемы. Хотя и существуют примеры для более сложных моделей, например, Attention-слои, при которых мы можем судить о важности определенных токенов при генерации предсказания.
Post-hoc же методы предоставляют объяснения только после того как модель обучена и зачастую эти методы агностичны к рассматриваемой модели.

Также методы интерпретации различаются и по виду их результата. Это может быть обобщающая статистика по входным признакам (пример - feature importance в случайном лесе), некоторая обобщающая визуализация по входным признакам, интерпретация по весам модели (для линейной регрессии и решающего дерева), некоторое противоположное или похожее наблюдение (контрфакты и adversarial-примеры) или аппроксимация рассматриваемой модели через более простую внутренне интерпретируемую модель.

% TODO
% Предложенная группировка методы следует \cite{madsenPosthocInterpretabilityNeural2021}


\vspace{15mm}
\section{Локальные объяснения}

\subsection{Входные признаки}

В данной группы методов мы пытаемся понять, насколько важны были те или иные признаки на входе для заданного предсказания. В случае NLP-моделей мы "подсвечиваем" важные токены текста на входе, в случае CV-моделей - пиксели заданного изображения.

Классическим представителем является градиентный метод \cite{baehrensHowExplainIndividual2009}, в котором для классификатора мерой важности признака является значение градиента.

Развитием градиентного метода является специфичный для нейросетей метод \textbf{DeepLIFT} \cite{shrikumarLearningImportantFeatures2019}, который заключается в сравнении работы нейронов с некоторым "примерным" наблюдением. В случае изображения это может быть серое изображение, в случае текста - пустой текст из [PAD] токенов. 

Другим популярным методом является \textbf{LIME} \cite{ribeiroWhyShouldTrust2016}. Суть его заключается в семплировании похожих объектов и и обучения на их предсказаниях логистической регрессии. Чем больше параметры локальной логистической регресии - тем важнее этот объект. В NLP объекты семплируются с помощью BoW с косинусным расстоянием и с помощью маскирования токенов. Для изображений используются суперпиксели.

Проблемой LIME является то, что при наличии мультиколлинеарности признаков значения весов линейной модели становится трудно интерпретировать. В методе \textbf{SHAP} \cite{lundbergUnifiedApproachInterpreting2017} используются теоретические наработки с векторами Шепли для решения этой проблемы. От этого появляется другой недостаток - для работы SHAP требуется оценить большое количество сгенерированных наблюдений.

Существует критика таких подходов, которая заключается в том, что показанная важность определенных токенов может не отражать реальную значимость для предсказаний. В частности, \cite{pruthiLearningDeceiveAttentionBased2020a} показан метод, как можно сфабриковать нужные важности токенов используя Attention-слои. Также \cite{slackFoolingLIMESHAP2020} удалось для LIME и SHAP подделать результаты, чтобы скрыть реальную работу модели.

\vspace{8mm}
\subsection{Adversarial примеры}

В этой категории методов мы пытаемся "обмануть" модель, подобрав похожий объект, на котором она выдала бы другое предсказание. Adversarial-атаки произошли из области исследований, посвященной теме устойчивости моделей. Однако дает нам некоторое понимание работы модели.

В \cite{millerExplanationArtificialIntelligence2019} утверждается, что контрастные суждения о модели, которые сравнивают заданный пример с другим, более понятны для человека, чем важность признаков.

Примеры adversarial методов:
\begin{itemize}
    \item \textbf{HotFlip} \cite{ebrahimiHotFlipWhiteBoxAdversarial2018}. В этом методе изменяются токены в предложении так, чтобы максимизировать изменение функции потерь. В качестве меры важности токенов используется величина градиента при изменении. Процедура изменения одного токена применяется на одном и том же предложении множество раз с помощью beam search процедуры. В оригинальной статье предлагается интерпретация модели, работающей на символьном уровне, но метод можно адаптировать на любые последовательности, подбирая кандидатов на замену с помощью косинусного расстояния.
    \item \textbf{Semantically Equivalent Adversaries (SEA)} \cite{ribeiroSemanticallyEquivalentAdversarial2018}. Используется метрика семантической эквивалентности текстов, которая вычисляется как условное правдоподобие текста относительно оригинального текста. Наиболее похожий по этой метрике текст заменяется и процедура может повторяться множество раз, пока не будет удовлетворен критерий останова. Важно отметить различие от HotFlip метода, парафразирующая модель может удалять, добавлять и изменять сразу множество токенов за раз.
    \item \textbf{BERT-ATTACK} \cite{liBERTATTACKAdversarialAttack2020} для генерации примеров использует специальная предобученную BERT модель. Генерация происходит в два этапа: BERT определяет наиболее уязвимые слова в заданном предложении и затем эти слова маскируются, чтобы BERT предсказал другие токены на их место. % Так как BERT изначально обучен на задаче Masked LM, генерация замещенных у
\end{itemize}

\vspace{8mm}
\subsection{Контрфакты (Counterfactuals)}

Пытаемся отредактировать вход так, чтобы поменялось предсказание. Counterfactual должны минимально отличаться от исходного примера.
Похожий по смыслу, на Adversarial примеры, Но в Adversarial примерах мы пытаемся подобрать парафразу, чтобы изменить предсказать, контрфакты должны семантически совпадать.

Примером метода генерации контрфактов является \textbf{Polyjuice} \cite{wuPolyjuiceGeneratingCounterfactuals2021}. Методика модель-агноситчна. Выполняется с помощью файнтюнинга GPT-2 на существующем датасете контрфактов, генерируя для обучения специально построенные предложения из пар оригинального и контрфактических примеров. Изучаемая модель взаимодействует с системой только на этапе фильтрации контрфактов - выбираются только те, которые значительно меняют предсказание модели.

В отличие от Polyjuice методика \textbf{MiCE} предложенная в \cite{rossExplainingNLPModels2021} более тесно работает с объясняемой моделью.

Для генерации контфрактов используется seq2sec модель, пытающаяся предсказать замаскированные слова по метке класса. Например, для положительной метки, модель должна для предложения "\textit{Фильм был [BLANK]}" предсказать  "\textit{хороший}". Токены для маскирования выбираются по их важности, определенной с помощью градиентного метода.

Для генерации контрфактов используется обученная модель, с подмененной меткой класса. Так, модель берет некоторый важный токен, маскирует его и заменяет на противоположный по смыслу. Процедура генерации повторяется множество раз на одном предложении с помощью beam search процедуры.

\vspace{8mm}
\subsection{Похожие примеры}

По заданному наблюдению, пытаемся найти похожие на него, с точки зрения модели. Эти методы дополнительно полезен тем, что мы берем объекты из датасета, что дает нам возможность также лучше понять рабочий датасет.

В \cite{hanExplainingBlackBox2020} для отслеживания похожих примеров используется классическая техника функций влиятельности, которая в состоит измерении изменении значении функции потерь при удалении некоторого объекта из обучающего набора данных. Эта техника позволяет отследить причины предсказания модели к её тренировочному датасету. В данной работе применение функции влиятельности было расширена на BERT модель.

\vspace{8mm}
\subsection{Объяснения на естественном языке}

Пытаемся дать объяснения в виде текста простым языком. При этом объяснения должны быть понятны не-экспертам. Интересной особенностью применения такого метода на NLP-моделях заключается в том, что сгенерированные объяснения могут использоваться для улучшения качества объясняемой модели.

В работе \cite{rajaniExplainYourselfLeveraging2019} представлена похожая методика CAGE (Commonsense Auto-Generated Explanations) для генерации ответов на вопросы с заданными вариантами ответов. К существующему QA датасету с помощью Mechanical Turk к каждой паре "вопрос-возможные ответы" было подобрано объяснения ответа. Например для вопроса "Можно заняться вязанием, чтобы почувствовать что?" с предлагаемыми ответами "спокойство" и "артрит", дается объяснение "Вязание позволяет успокоиться". На этом расширенном датасете дообучается GPT, чтобы он мог генерировать эти объяснения.

Авторы предлагают специальный режим рационализации, при котором обучение GPT дополнительно обусловлено предсказанным сторонней моделью ответом. Таким образом  GPT пытается объяснить полученное предсказание. Проблема состоит в том, что GPT очень слабо связана с исходной моделью - смысл методики состоит прежде всего в улучшении качества предсказаний с помощью сгенерированных предсказаний. Полученные объяснения в этом контексте лишь побочный продукт.

В \cite{latcinnikExplainingQuestionAnswering2020} используется похожий подход в решении задачи QA, при котором модель-генератор пытается предложить гипотетические ответы на предложенный вопрос, на которых модель-классификатор дополнительно обуславливается. 

% \section{Классовые объяснения}

% К этим методам можно отнести метод объяснения высокоуровневыми концептами. 
\vspace{15mm}
\section{Глобальные объяснения}

\subsection{Словарные объяснения}

В этих методах мы пытаемся понять модель через её словарь. Зачастую мы для этого изучаются вектора-эмбеддинги модели.

Одним из простых подходов к изучению эмбеддингов является их визуализация с помощью сокращения количества измерений векторов методом t-SNE. Такой метод был предложен в \cite{liVisualizingUnderstandingNeural2016}. Это позволяет увидеть некоторую структуру в словаре модели. Например, образуются отдельные кластеры семантически похожих слов.

\vspace{8mm}
\subsection{Ансамбли}

Этот тип производных методов, при которых мы пытается дать объяснение всей модели как комбинацию локальных объяснений. Технически, это можно выполнить любым представленным локальным методам, что делает эту категорию широкой. Очевидным недостатком этой категории заключается в дороговизне - получение одного локального объяснения может быть ёмкой задачей само по себе. Поэтому при работе с этими методами пытаются выбрать из датасета малое число наблюдений для интерпретации так, чтобы они могли лучше всего отобразить работу модели. 

Следует отметить метод, предложенные авторами LIME \cite{ribeiroWhyShouldTrust2016}, в котором производится попытка выбрать ограниченное число наблюдений, которые с помощью LIME метода могли бы лучше всего отразить работу модели.

\vspace{8mm}
\subsection{Лингвистическая информация}

Чтобы подтвердить разумность NLP модели, можно согласовать модель с обширной лингвистической теорией.

Методы в этой категории либо исследуют реакцию модели на специальные изменения входа или пытаются показать соответствия латентного и некоторого лингвистического представления. Первое называется поведенческим анализом, второе - структурным анализом

\vspace{8mm}
\subsection{Правила}

Эти методы пытаются представить обученную модель как набор простых правил.

В упомянутой ранее работе \cite{ribeiroSemanticallyEquivalentAdversarial2018} метод для генерации adversarial примеров SEA расширяется для последующей генерации правил. Для этого анализируются паттерны новых примеров, на которых модель часто начинает менять предсказание на неправильное.

В работе \cite{muCompositionalExplanationsNeurons2021} авторы пытаются объяснить формальными правилами представления отдельных нейроной. В случае изображений, для заданного нейрона и наблюдений собираются маски, на которых срабатывает нейрон. Также имеются набор атомарных концептов и для для всех изображений масками отмечено присутствие концептов на изображении. Для полученных масок активации нейронов собираются концепты, присутствующие на этих масках. Затем с помощью процедуры beam search собранные концепты и их области объединяются логическими операторами И, ИЛИ и НЕ так, чтобы как можно точнее покрыть области активации нейрона. Также авторы адоптируют этот подход  на задаче natural language inference (NLI).


