\chapter{Введение}
\addcontentsline{toc}{chapter}{Введение}  

Современные модели машинного обучения показывают хорошие результаты в своих предсказательных способностях, что их примение в реальных задачах стало обыденным явлением. Однако этого удалось добиться за счет повышения сложности моделей. Это привело к тому, что современные модели машинного обучения являются \textbf{черными ящиками} - системами, чье внутреннее устройство и процесс получения ответа являются исключительно трудными для анализа. В противовес прозрачным "простым" моделям вроде линейной регрессии и решающим деревьям, мы зачастую не можем достоверно достоверно ответить на вопрос "как работает эта система" в случае современных нейронных сетей.

Существуют опасения относительно надежности и оправданности предсказаний, которые дают современные ML-системы. Это особенно критичная проблема для областей, в которых принимаются решения с очень большими ставками: здравоохранение, правосудие, финансы и т. д.  Для этих областей, существуют повышенные требования к безопасности, этичности и ответственности принимаемых решений. Уже существует обильное множество примеров реальных нейросетевых моделей, которые показывают нежелательные поведения и возникающие с этим этические проблемы. \todo{Добавить статей с примерами}

Проблема в удовлетворении к возросшим требованиям к безопасности систем машинного обучения в том, что мы не можем  доподлинно покрыть юнит-тестами все возможные поведения модели, как это возможно в сфере разработки программного обеспечения. Чтобы убедиться в надежности и безопасности модели вместо этого приходится полагаться на интерпретируемость. Если модель интерпретируема для нас, то мы опосредованно можем судить о её надежноссти.

К сожалению, общепринятого формального определения интерпретации не существует. В контексте ML систем в \hl{doshi-velez_towards_2017} дается определение интерпретируемости как "способность объяснить или представить в понятных человеку терминах".

% TODO

\section{Мотивация}

A fundamental motivation for interpretability is \textbf{accountability}. For example, if a predictive mistake happens which caused harm, it’s important to explain why this mistake happened \hl{doshi-velez_towards_2017}. Similarly, for high-stakes decisions, it’s important to minimize the risk of model failure by explaining the model before deployment (Rudin, 2019). In other words, it is important to distinguish between when interpretability is applied **proactively** or **retroactively** to the model’s deployment 

hl{Because of the inherent ability to explain the model after training, post-hoc methods are valuable in legal proceedings, where models may need to be explained retroactively (Doshi-Velez et al., 2017). Additionally, they fit into existing quality assessment structures, such as those used to regulate banking, where quality assessment is also done after a model has been built (Bhatt et al., 2019)}

В \cite{madsen_post-hoc_2021} \hl{doshi-velez_towards_2017} обощаются следующие ключевых фактора мотивации развития методов интерпретируемости:

\begin{itemize}
    \item \textbf{Этичность}. Подразумевает, что поведение модели согласуется с представлениями общества об этике и морали. Так как в целом понятие "этичности поведения" hl{трудно оценить и представить объективную метрику} для этого, обычно это оценивается вручную специально назначенными людьми. Типично для рассмотрения таких вопрос в крупных компаниях существуют целые отделы по этике.
    Типичный вопрос представляющий опасения об этичности  модели - проблема дискримации. И уже существует ряд работ, посвященные формализации метрики "справедливости" (fairness) и методам устранения нежелательных предубеждений. Но так как это является лишь одним из многих возможных этических проблем, нужда во вручной человеческой оценке остается.
    \item \textbf{Безопасность}. Обозначает то, что модель работает в рамках наших ожиданий. Зачастую подразумевает собой также устойчивость к adversarial атакаи и к смещению распределений. В более широком смысле \cite{lipton_mythos_2018} под этим также подразумевается доверие к модели.
    \item \textbf{Подотчетность}. Обозначает способность модели "объясниться" в случае  \hl{relates to explaining the model when it does fail in production. The “right to explanation”, regarding the logic involved in the model’s prediction, is increasingly being adopted, most notably in the EU via its **[[GDPR legislation]]**. Additionally, industries such as banking, are already required to audit their models}
    \item \textbf{Научный интерес} \hl{addresses a need by researchers and scientists, which is to generate hypotheses and knowledge. As Doshi-Velez and Kim (2017) frames it, sometimes the best way to start such a process is to ask for explanations. In model development, explanations can also be useful for **model debugging** (Bhatt et al., 2019), which is often facilitated by the same kinds of explanations.}
\end{itemize}

% \section{Актуальность}

% Про GDPR и повышенные требования к безопасности

