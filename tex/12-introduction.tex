\chapter{Введение}
\addcontentsline{toc}{chapter}{Введение}  

Современные модели машинного обучения показывают хорошие результаты в своих предсказательных способностях, что их применение в реальных задачах стало обыденным явлением. Однако этого удалось добиться за счет повышения сложности моделей. Это привело к тому, что современные модели машинного обучения являются \textbf{черными ящиками} - системами, чье внутреннее устройство и процесс получения ответа являются исключительно трудными для анализа. В противовес прозрачным "простым" моделям вроде линейной регрессии и решающим деревьям, мы зачастую не можем достоверно достоверно ответить на вопрос "как работает эта система" в случае современных нейронных сетей.

Существуют опасения относительно надежности и оправданности предсказаний, которые дают современные ML-системы. Это особенно критичная проблема для областей, в которых принимаются решения с очень большой ответственностью: здравоохранение, правосудие, финансы и т. д.  Для этих областей, существуют повышенные требования к безопасности, этичности и ответственности принимаемых решений. Уже существует обильное множество примеров реальных нейросетевых моделей, которые показывают нежелательные поведения и возникающие с этим этические проблемы.

Проблема в удовлетворении к возросшим требованиям к безопасности систем машинного обучения в том, что мы не можем  доподлинно покрыть юнит-тестами все возможные поведения модели, как это возможно в сфере разработки программного обеспечения. Чтобы убедиться в надежности и безопасности модели вместо этого приходится полагаться на интерпретируемость. Если модель интерпретируема для нас, то мы опосредованно можем судить о её надежности.

К сожалению, общепринятого формального определения интерпретации не существует. В контексте ML систем в \cite{doshi-velezRigorousScienceInterpretable2017}  дается определение интерпретируемости как "способность объяснить или представить в понятных человеку терминах".

% TODO

\section{Мотивация}

В \cite{madsenPosthocInterpretabilityNeural2021} утверждается, что всю мотивацию интерпретируемости моделей машинного обучения можно свести к \textbf{отвественности} за их решения. В случае ошибочного предсказания модели нам необходимо понимать, почему эта ошибка произошла. А в случае когда цена ошибки модели слишком высока, мы хотим минимизировать возможные риски модели перед её развертыванием. % (Rudin, 2019).
Этими двумя случаями мы описали ситуации, когда мы применяем интерпретацию \textbf{ретроактивно} и \textbf{проактивно} соответственно. Ретроактивные интерпретации получили большое распространение благодаря возможности использовать их в судопроизводстве и в уже существующих структурах оценки и контроля качества в банковской среде. % TODO (Bhatt et al., 2019)

В \cite{madsenPosthocInterpretabilityNeural2021} \cite{doshi-velezRigorousScienceInterpretable2017} обобщаются следующие ключевых фактора мотивации развития методов интерпретируемости:

% Заменить ссылки

\begin{itemize}
    \item \textbf{Этичность}. Подразумевает, что поведение модели согласуется с представлениями общества об этике и морали. Так как в целом понятие "этичности поведения" {трудно оценить и представить объективную метрику} для этого, обычно это оценивается вручную специально назначенными людьми. Типично для рассмотрения таких вопрос в крупных компаниях существуют целые отделы по этике.
    Типичный вопрос представляющий опасения об этичности  модели - проблема дискриминации. И уже существует ряд работ, посвященные формализации метрики "справедливости" (fairness) и методам устранения нежелательных предубеждений. Но так как это является лишь одним из многих возможных этических проблем, нужда во в ручной человеческой оценке остается.
    \item \textbf{Безопасность}. Обозначает то, что модель работает в рамках наших ожиданий. Зачастую подразумевает собой также устойчивость к adversarial атакам и к смещению распределений. В более широком смысле \cite{liptonMythosModelInterpretability2018} под этим также подразумевается доверие к модели.
    \item \textbf{Подотчетность}. Обозначает способность модели "объясниться" в случае неудачи в production-среде. Требование к способности модели объяснить свое решение сейчас все больше и больше распространяется - сейчас это проявляется в виде постановления GDPR от Европейского Союза и банковской среде.
    \item \textbf{Научный интерес}. Помимо простого удовлетворения человеческой жажды знаний со стороны исследователей, скрывает за собой также более приземленную причину. Если мы понимаем, почему модель должна работать, мы также можем ответить и на вопрос "почему наша конкретная модель \textif{не} работает как задумано". То есть, методы интерпретирования моделей очень полезны для процесса дебагинга моделей % (Bhatt et al., 2019)
\end{itemize}

% \section{Актуальность}

% Про GDPR и повышенные требования к безопасности

