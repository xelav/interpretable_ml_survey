
@article{madsenPosthocInterpretabilityNeural2021,
title = {Post-hoc {Interpretability} for {Neural} {NLP}: {A} {Survey}},
shorttitle = {Post-hoc {Interpretability} for {Neural} {NLP}},
url = {http://arxiv.org/abs/2108.04840},
abstract = {Natural Language Processing (NLP) models have become increasingly more complex and widespread. With recent developments in neural networks, a growing concern is whether it is responsible to use these models. Concerns such as safety and ethics can be partially addressed by providing explanations. Furthermore, when models do fail, providing explanations is paramount for accountability purposes. To this end, interpretability serves to provide these explanations in terms that are understandable to humans. Central to what is understandable is how explanations are communicated. Therefore, this survey provides a categorization of how recent interpretability methods communicate explanations and discusses the methods in depth. Furthermore, the survey focuses on post-hoc methods, which provide explanations after a model is learned and generally model-agnostic. A common concern for this class of methods is whether they accurately reflect the model. Hence, how these post-hoc methods are evaluated is discussed throughout the paper.},
urldate = {2021-10-28},
journal = {arXiv:2108.04840 [cs]},
author = {Madsen, Andreas and Reddy, Siva and Chandar, Sarath},
month = aug,
year = {2021},
note = {arXiv: 2108.04840},
keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
file = {arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\CMGIH57X\\2108.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\VFUJHGWH\\Madsen и др. - 2021 - Post-hoc Interpretability for Neural NLP A Survey.pdf:application/pdf},
}

@article{alshargiConcept2vecMetricsEvaluating2020,
title = {Concept2vec: {Metrics} for {Evaluating} {Quality} of {Embeddings} for {Ontological} {Concepts}},
shorttitle = {Concept2vec},
url = {http://arxiv.org/abs/1803.04488},
abstract = {Although there is an emerging trend towards generating embeddings for primarily unstructured data and, recently, for structured data, no systematic suite for measuring the quality of embeddings has been proposed yet. This deficiency is further sensed with respect to embeddings generated for structured data because there are no concrete evaluation metrics measuring the quality of the encoded structure as well as semantic patterns in the embedding space. In this paper, we introduce a framework containing three distinct tasks concerned with the individual aspects of ontological concepts: (i) the categorization aspect, (ii) the hierarchical aspect, and (iii) the relational aspect. Then, in the scope of each task, a number of intrinsic metrics are proposed for evaluating the quality of the embeddings. Furthermore, w.r.t. this framework, multiple experimental studies were run to compare the quality of the available embedding models. Employing this framework in future research can reduce misjudgment and provide greater insight about quality comparisons of embeddings for ontological concepts. We positioned our sampled data and code at https://github.com/alshargi/Concept2vec under GNU General Public License v3.0.},
urldate = {2021-11-14},
journal = {arXiv:1803.04488 [cs]},
author = {Alshargi, Faisal and Shekarpour, Saeedeh and Soru, Tommaso and Sheth, Amit},
month = may,
year = {2020},
note = {arXiv: 1803.04488},
keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Concept2Vec, I.2.4, I.2.6},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\II7AMTJU\\Alshargi и др. - 2020 - Concept2vec Metrics for Evaluating Quality of Emb.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\WNB2KHDQ\\1803.html:text/html;concept2vec.md:C\:\\Users\\Admin\\Dropbox\\Vaults\\interpretable-nlp\\Interpretable NLP\\Concept2Vec\\concept2vec.md:text/plain},
}

@article{alatrishBuildingOntologiesDifferent2014,
title = {Building ontologies for different natural languages},
volume = {11},
issn = {1820-0214, 2406-1018},
url = {http://www.doiserbia.nb.rs/Article.aspx?ID=1820-02141400023A},
doi = {10.2298/CSIS130429023A},
abstract = {Ontology construction of a certain domain is an important step in applying the Semantic web. A number of software tools adapted for building domain ontologies of most wide–spread natural languages are available, but accomplishing that for any given natural language presents a challenge. Here we propose a semi-automatic procedure to create ontologies for different natural languages. Our approach utilizes various software tools available on the Internet most notably DODDLE-OWL - a domain ontology development tool implemented for English and Japanese languages. By using this tool, WordNet, Protégé and XSLT transformations, we propose a general procedure to construct domain ontology for any natural language.},
language = {en},
number = {2},
urldate = {2021-11-14},
journal = {Computer Science and Information Systems},
author = {Alatrish, Emhimed and Tosic, Dusan and Milenkovic, Nikola},
year = {2014},
pages = {623--644},
file = {Alatrish и др. - 2014 - Building ontologies for different natural language.pdf:C\:\\Users\\Admin\\Zotero\\storage\\WV39J93V\\Alatrish и др. - 2014 - Building ontologies for different natural language.pdf:application/pdf},
}

@incollection{auerDBpediaNucleusWeb2007,
address = {Berlin, Heidelberg},
title = {{DBpedia}: {A} {Nucleus} for a {Web} of {Open} {Data}},
volume = {4825},
isbn = {978-3-540-76297-3 978-3-540-76298-0},
shorttitle = {{DBpedia}},
url = {http://link.springer.com/10.1007/978-3-540-76298-0_52},
abstract = {DBpedia is a community eﬀort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human- and machine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.},
language = {en},
urldate = {2021-11-14},
booktitle = {The {Semantic} {Web}},
publisher = {Springer Berlin Heidelberg},
author = {Auer, Sören and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Aberer, Karl and Choi, Key-Sun and Noy, Natasha and Allemang, Dean and Lee, Kyung-Il and Nixon, Lyndon and Golbeck, Jennifer and Mika, Peter and Maynard, Diana and Mizoguchi, Riichiro and Schreiber, Guus and Cudré-Mauroux, Philippe},
year = {2007},
doi = {10.1007/978-3-540-76298-0_52},
note = {Series Title: Lecture Notes in Computer Science},
pages = {722--735},
file = {Auer и др. - 2007 - DBpedia A Nucleus for a Web of Open Data.pdf:C\:\\Users\\Admin\\Zotero\\storage\\MHCXRR68\\Auer и др. - 2007 - DBpedia A Nucleus for a Web of Open Data.pdf:application/pdf;DBpedia-article.md:C\:\\Users\\Admin\\Dropbox\\Vaults\\interpretable-nlp\\Interpretable NLP\\Concept2Vec\\DBpedia-article.md:text/plain},
}

@article{doshi-velezRigorousScienceInterpretable2017,
title = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
url = {http://arxiv.org/abs/1702.08608},
abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
urldate = {2021-11-17},
journal = {arXiv:1702.08608 [cs, stat]},
author = {Doshi-Velez, Finale and Kim, Been},
month = mar,
year = {2017},
note = {arXiv: 1702.08608},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\Q3SSDAR9\\Doshi-Velez и Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\QH9HISC7\\1702.html:text/html;Towards a Rigorous Science of Interpretable Machine Learning:C\:\\Users\\Admin\\Dropbox\\Vaults\\interpretable-nlp\\Papers To Read\\Towards a Rigorous Science of Interpretable Machine Learning.md:text/plain},
}

@article{liptonMythosModelInterpretability2018,
title = {The mythos of model interpretability},
volume = {61},
issn = {0001-0782},
url = {https://doi.org/10.1145/3233231},
doi = {10.1145/3233231},
abstract = {In machine learning, the concept of interpretability is both important and slippery.},
number = {10},
urldate = {2021-11-17},
journal = {Communications of the ACM},
author = {Lipton, Zachary C.},
month = sep,
year = {2018},
pages = {36--43},
file = {Отправленная версия:C\:\\Users\\Admin\\Zotero\\storage\\XLESK2PE\\Lipton - 2018 - The mythos of model interpretability.pdf:application/pdf;Lipton 2018.md:C\:\\Users\\Admin\\Dropbox\\Vaults\\interpretable-nlp\\Papers\\Papers To Read\\Lipton 2018.md:text/plain},
}

@article{laiOntologybasedInterpretableMachine2020,
title = {Ontology-based {Interpretable} {Machine} {Learning} for {Textual} {Data}},
url = {http://arxiv.org/abs/2004.00204},
abstract = {In this paper, we introduce a novel interpreting framework that learns an interpretable model based on an ontology-based sampling technique to explain agnostic prediction models. Different from existing approaches, our algorithm considers contextual correlation among words, described in domain knowledge ontologies, to generate semantic explanations. To narrow down the search space for explanations, which is a major problem of long and complicated text data, we design a learnable anchor algorithm, to better extract explanations locally. A set of regulations is further introduced, regarding combining learned interpretable representations with anchors to generate comprehensible semantic explanations. An extensive experiment conducted on two real-world datasets shows that our approach generates more precise and insightful explanations compared with baseline approaches.},
urldate = {2021-11-17},
journal = {arXiv:2004.00204 [cs, stat]},
author = {Lai, Phung and Phan, NhatHai and Hu, Han and Badeti, Anuja and Newman, David and Dou, Dejing},
month = mar,
year = {2020},
note = {arXiv: 2004.00204},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\7ZLRVKV6\\Lai и др. - 2020 - Ontology-based Interpretable Machine Learning for .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\LX49TDIE\\2004.html:text/html},
}

@article{chengImprovingDisentangledText2020,
title = {Improving {Disentangled} {Text} {Representation} {Learning} with {Information}-{Theoretic} {Guidance}},
url = {http://arxiv.org/abs/2006.00693},
abstract = {Learning disentangled representations of natural language is essential for many NLP tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc. Similar problems have been studied extensively for other forms of data, such as images and videos. However, the discrete nature of natural language makes the disentangling of textual representations more challenging (e.g., the manipulation over the data space cannot be easily achieved). Inspired by information theory, we propose a novel method that effectively manifests disentangled representations of text, without any supervision on semantics. A new mutual information upper bound is derived and leveraged to measure dependence between style and content. By minimizing this upper bound, the proposed method induces style and content embeddings into two independent low-dimensional spaces. Experiments on both conditional text generation and text-style transfer demonstrate the high quality of our disentangled representation in terms of content and style preservation.},
urldate = {2021-11-17},
journal = {arXiv:2006.00693 [cs, stat]},
author = {Cheng, Pengyu and Min, Martin Renqiang and Shen, Dinghan and Malon, Christopher and Zhang, Yizhe and Li, Yitong and Carin, Lawrence},
month = jun,
year = {2020},
note = {arXiv: 2006.00693},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\4PWUQYIE\\Cheng и др. - 2020 - Improving Disentangled Text Representation Learnin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\4WANT5X8\\2006.html:text/html;Cheng (2020).md:C\:\\Users\\Admin\\Dropbox\\Vaults\\interpretable-nlp\\Papers\\Papers To Read\\Cheng (2020).md:text/plain},
}

@article{zhangDisentanglingRepresentationsText2021,
title = {Disentangling {Representations} of {Text} by {Masking} {Transformers}},
url = {http://arxiv.org/abs/2104.07155},
abstract = {Representations from large pretrained models such as BERT encode a range of features into monolithic vectors, affording strong predictive accuracy across a multitude of downstream tasks. In this paper we explore whether it is possible to learn disentangled representations by identifying existing subnetworks within pretrained models that encode distinct, complementary aspect representations. Concretely, we learn binary masks over transformer weights or hidden units to uncover subsets of features that correlate with a specific factor of variation; this eliminates the need to train a disentangled model from scratch for a particular task. We evaluate this method with respect to its ability to disentangle representations of sentiment from genre in movie reviews, "toxicity" from dialect in Tweets, and syntax from semantics. By combining masking with magnitude pruning we find that we can identify sparse subnetworks within BERT that strongly encode particular aspects (e.g., toxicity) while only weakly encoding others (e.g., race). Moreover, despite only learning masks, we find that disentanglement-via-masking performs as well as -- and often better than -- previously proposed methods based on variational autoencoders and adversarial training.},
urldate = {2021-11-17},
journal = {arXiv:2104.07155 [cs]},
author = {Zhang, Xiongyi and van de Meent, Jan-Willem and Wallace, Byron C.},
month = sep,
year = {2021},
note = {arXiv: 2104.07155},
keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\ZT92TSE6\\Zhang и др. - 2021 - Disentangling Representations of Text by Masking T.pdf:application/pdf;Zhang (2021).md:C\:\\Users\\Admin\\Dropbox\\Vaults\\interpretable-nlp\\Papers\\Papers To Read\\Zhang (2021).md:text/plain;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\H9YJUHWS\\2104.html:text/html},
}

@article{bolukbasiInterpretabilityIllusionBERT2021,
title = {An {Interpretability} {Illusion} for {BERT}},
url = {http://arxiv.org/abs/2104.07143},
abstract = {We describe an "interpretability illusion" that arises when analyzing the BERT model. Activations of individual neurons in the network may spuriously appear to encode a single, simple concept, when in fact they are encoding something far more complex. The same effect holds for linear combinations of activations. We trace the source of this illusion to geometric properties of BERT's embedding space as well as the fact that common text corpora represent only narrow slices of possible English sentences. We provide a taxonomy of model-learned concepts and discuss methodological implications for interpretability research, especially the importance of testing hypotheses on multiple data sets.},
urldate = {2021-11-23},
journal = {arXiv:2104.07143 [cs]},
author = {Bolukbasi, Tolga and Pearce, Adam and Yuan, Ann and Coenen, Andy and Reif, Emily and Viégas, Fernanda and Wattenberg, Martin},
month = apr,
year = {2021},
note = {arXiv: 2104.07143},
keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\EAGJVUTX\\Bolukbasi и др. - 2021 - An Interpretability Illusion for BERT.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\QNUUGSDT\\2104.html:text/html},
}

@article{shekarpourCEVOComprehensiveEVent2018,
title = {{CEVO}: {Comprehensive} {EVent} {Ontology} {Enhancing} {Cognitive} {Annotation}},
shorttitle = {{CEVO}},
url = {http://arxiv.org/abs/1701.05625},
abstract = {While the general analysis of named entities has received substantial research attention on unstructured as well as structured data, the analysis of relations among named entities has received limited focus. In fact, a review of the literature revealed a deficiency in research on the abstract conceptualization required to organize relations. We believe that such an abstract conceptualization can benefit various communities and applications such as natural language processing, information extraction, machine learning, and ontology engineering. In this paper, we present Comprehensive EVent Ontology (CEVO), built on Levin's conceptual hierarchy of English verbs that categorizes verbs with shared meaning, and syntactic behavior. We present the fundamental concepts and requirements for this ontology. Furthermore, we present three use cases employing the CEVO ontology on annotation tasks: (i) annotating relations in plain text, (ii) annotating ontological properties, and (iii) linking textual relations to ontological properties. These use-cases demonstrate the benefits of using CEVO for annotation: (i) annotating English verbs from an abstract conceptualization, (ii) playing the role of an upper ontology for organizing ontological properties, and (iii) facilitating the annotation of text relations using any underlying vocabulary. This resource is available at https://shekarpour.github.io/cevo.io/ using https://w3id.org/cevo namespace.},
urldate = {2021-11-23},
journal = {arXiv:1701.05625 [cs]},
author = {Shekarpour, Saeedeh and Alshargi, Faisal and Shalin, Valerie and Thirunarayan, Krishnaprasad and Sheth, Amit P.},
month = oct,
year = {2018},
note = {arXiv: 1701.05625
version: 2},
keywords = {Computer Science - Computation and Language},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\YHMVN7W4\\Shekarpour и др. - 2018 - CEVO Comprehensive EVent Ontology Enhancing Cogni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\GBV2T2UG\\1701.html:text/html},
}

@article{paccanaroLearningDistributedRepresentations2000,
title = {Learning distributed representations of concepts from relational data using linear relational embedd},
journal = {IEEE Transactions on Knowledge and Data Engineering - TKDE},
author = {Paccanaro, Alberto and Hinton, G.},
month = jan,
year = {2000},
}

@misc{heInterpretableNLP2021,
title = {Interpretable {NLP}},
copyright = {MIT},
url = {https://github.com/ShilinHe/interpretableNLP},
abstract = {A list of publications on NLP interpretability (Welcome PR)},
urldate = {2021-11-23},
author = {HE, Shilin},
month = nov,
year = {2021},
note = {original-date: 2019-07-11T01:35:34Z},
}

@article{pruthiLearningDeceiveAttentionBased2020,
title = {Learning to {Deceive} with {Attention}-{Based} {Explanations}},
url = {http://arxiv.org/abs/1909.07913},
abstract = {Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention's reliability as a tool for auditing algorithms in the context of fairness and accountability.},
urldate = {2021-11-23},
journal = {arXiv:1909.07913 [cs]},
author = {Pruthi, Danish and Gupta, Mansi and Dhingra, Bhuwan and Neubig, Graham and Lipton, Zachary C.},
month = apr,
year = {2020},
note = {arXiv: 1909.07913},
keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\EUQSJKLS\\Pruthi и др. - 2020 - Learning to Deceive with Attention-Based Explanati.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\WYHUE8EZ\\1909.html:text/html},
}

@article{heDeBERTaDecodingenhancedBERT2021,
title = {{DeBERTa}: {Decoding}-enhanced {BERT} with {Disentangled} {Attention}},
shorttitle = {{DeBERTa}},
url = {http://arxiv.org/abs/2006.03654},
abstract = {Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9\% (90.2\% vs. 91.1\%), on SQuAD v2.0 by +2.3\% (88.4\% vs. 90.7\%) and RACE by +3.6\% (83.2\% vs. 86.8\%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).},
urldate = {2021-11-24},
journal = {arXiv:2006.03654 [cs]},
author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
month = oct,
year = {2021},
note = {arXiv: 2006.03654},
keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, cs.CL, cs.GL, I.2, I.7},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\MYURRFX4\\He и др. - 2021 - DeBERTa Decoding-enhanced BERT with Disentangled .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\L7MQBA32\\2006.html:text/html;undefined-zotero.md:C\:\\Users\\Admin\\Dropbox\\Vaults\\interpretable-nlp\\Papers\\undefined-zotero.md:text/plain},
}

@article{wuPolyjuiceGeneratingCounterfactuals2021,
title = {Polyjuice: {Generating} {Counterfactuals} for {Explaining}, {Evaluating}, and {Improving} {Models}},
shorttitle = {Polyjuice},
url = {http://arxiv.org/abs/2101.00288},
abstract = {While counterfactual examples are useful for analysis and training of NLP models, current generation methods either rely on manual labor to create very few counterfactuals, or only instantiate limited types of perturbations such as paraphrases or word substitutions. We present Polyjuice, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning GPT-2 on multiple datasets of paired sentences. We show that Polyjuice produces diverse sets of realistic counterfactuals, which in turn are useful in various distinct applications: improving training and evaluation on three different tasks (with around 70\% less annotation effort than manual generation), augmenting state-of-the-art explanation techniques, and supporting systematic counterfactual error analysis by revealing behaviors easily missed by human experts.},
urldate = {2021-11-26},
journal = {arXiv:2101.00288 [cs]},
author = {Wu, Tongshuang and Ribeiro, Marco Tulio and Heer, Jeffrey and Weld, Daniel S.},
month = jun,
year = {2021},
note = {arXiv: 2101.00288},
keywords = {Computer Science - Computation and Language},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\AYKAVFJ8\\Wu и др. - 2021 - Polyjuice Generating Counterfactuals for Explaini.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\393SEYRB\\2101.html:text/html;undefined-zotero.md:C\:\\Users\\Admin\\Dropbox\\Vaults\\interpretable-nlp\\Papers\\__Papers To Read\\undefined-zotero.md:text/plain},
}

@book{molnarInterpretableMachineLearning,
title = {Interpretable {Machine} {Learning}},
url = {https://christophm.github.io/interpretable-ml-book/index.html},
abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
urldate = {2021-12-11},
author = {Molnar, Christoph},
file = {Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\R79DF8TW\\index.html:text/html},
}

@article{pfauRobustSemanticInterpretability2021,
title = {Robust {Semantic} {Interpretability}: {Revisiting} {Concept} {Activation} {Vectors}},
shorttitle = {Robust {Semantic} {Interpretability}},
url = {http://arxiv.org/abs/2104.02768},
abstract = {Interpretability methods for image classification assess model trustworthiness by attempting to expose whether the model is systematically biased or attending to the same cues as a human would. Saliency methods for feature attribution dominate the interpretability literature, but these methods do not address semantic concepts such as the textures, colors, or genders of objects within an image. Our proposed Robust Concept Activation Vectors (RCAV) quantifies the effects of semantic concepts on individual model predictions and on model behavior as a whole. RCAV calculates a concept gradient and takes a gradient ascent step to assess model sensitivity to the given concept. By generalizing previous work on concept activation vectors to account for model non-linearity, and by introducing stricter hypothesis testing, we show that RCAV yields interpretations which are both more accurate at the image level and robust at the dataset level. RCAV, like saliency methods, supports the interpretation of individual predictions. To evaluate the practical use of interpretability methods as debugging tools, and the scientific use of interpretability methods for identifying inductive biases (e.g. texture over shape), we construct two datasets and accompanying metrics for realistic benchmarking of semantic interpretability methods. Our benchmarks expose the importance of counterfactual augmentation and negative controls for quantifying the practical usability of interpretability methods.},
urldate = {2021-12-11},
journal = {arXiv:2104.02768 [cs, stat]},
author = {Pfau, Jacob and Young, Albert T. and Wei, Jerome and Wei, Maria L. and Keiser, Michael J.},
month = apr,
year = {2021},
note = {arXiv: 2104.02768},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\3J7547IT\\Pfau et al. - 2021 - Robust Semantic Interpretability Revisiting Conce.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\AFEJHQUR\\2104.html:text/html},
}

@article{federCausaLMCausalModel2021,
title = {{CausaLM}: {Causal} {Model} {Explanation} {Through} {Counterfactual} {Language} {Models}},
issn = {0891-2017, 1530-9312},
shorttitle = {{CausaLM}},
url = {http://arxiv.org/abs/2005.13407},
doi = {10.1162/coli_a_00404},
abstract = {Understanding predictions made by deep neural networks is notoriously difficult, but also crucial to their dissemination. As all machine learning based methods, they are as good as their training data, and can also capture unwanted biases. While there are tools that can help understand whether such biases exist, they do not distinguish between correlation and causation, and might be ill-suited for text-based models and for reasoning about high level language concepts. A key problem of estimating the causal effect of a concept of interest on a given model is that this estimation requires the generation of counterfactual examples, which is challenging with existing generation technology. To bridge that gap, we propose CausaLM, a framework for producing causal model explanations using counterfactual language representation models. Our approach is based on fine-tuning of deep contextualized embedding models with auxiliary adversarial tasks derived from the causal graph of the problem. Concretely, we show that by carefully choosing auxiliary adversarial pre-training tasks, language representation models such as BERT can effectively learn a counterfactual representation for a given concept of interest, and be used to estimate its true causal effect on model performance. A byproduct of our method is a language representation model that is unaffected by the tested concept, which can be useful in mitigating unwanted bias ingrained in the data.},
urldate = {2021-12-11},
journal = {Computational Linguistics},
author = {Feder, Amir and Oved, Nadav and Shalit, Uri and Reichart, Roi},
month = may,
year = {2021},
note = {arXiv: 2005.13407},
keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
pages = {1--54},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\NUKJYV6U\\Feder et al. - 2021 - CausaLM Causal Model Explanation Through Counterf.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\NYU4AV4F\\2005.html:text/html},
}

@article{baehrensHowExplainIndividual,
title = {How to {Explain} {Individual} {Classiﬁcation} {Decisions}},
abstract = {After building a classiﬁer with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most inﬂuential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classiﬁcation method.},
language = {en},
author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja},
pages = {29},
file = {Baehrens et al. - How to Explain Individual Classiﬁcation Decisions.pdf:C\:\\Users\\Admin\\Zotero\\storage\\JEY24Y2Q\\Baehrens et al. - How to Explain Individual Classiﬁcation Decisions.pdf:application/pdf},
}

@article{millerExplanationArtificialIntelligence2019,
title = {Explanation in artificial intelligence: {Insights} from the social sciences},
volume = {267},
issn = {00043702},
shorttitle = {Explanation in artificial intelligence},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370218305988},
doi = {10.1016/j.artint.2018.07.007},
abstract = {There has been a recent resurgence in the area of explainable artiﬁcial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artiﬁcial intelligence. However, it is fair to say that most work in explainable artiﬁcial intelligence uses only the researchers’ intuition of what constitutes a ‘good’ explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people deﬁne, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the ﬁeld of explainable artiﬁcial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important ﬁndings, and discusses ways that these can be infused with work on explainable artiﬁcial intelligence.},
language = {en},
urldate = {2021-12-12},
journal = {Artificial Intelligence},
author = {Miller, Tim},
month = feb,
year = {2019},
pages = {1--38},
file = {Miller - 2019 - Explanation in artificial intelligence Insights f.pdf:C\:\\Users\\Admin\\Zotero\\storage\\AKPVD3XX\\Miller - 2019 - Explanation in artificial intelligence Insights f.pdf:application/pdf},
}

@article{ebrahimiHotFlipWhiteBoxAdversarial2018,
title = {{HotFlip}: {White}-{Box} {Adversarial} {Examples} for {Text} {Classification}},
shorttitle = {{HotFlip}},
url = {http://arxiv.org/abs/1712.06751},
abstract = {We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.},
urldate = {2021-12-12},
journal = {arXiv:1712.06751 [cs]},
author = {Ebrahimi, Javid and Rao, Anyi and Lowd, Daniel and Dou, Dejing},
month = may,
year = {2018},
note = {arXiv: 1712.06751},
keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\83ZYUCRA\\Ebrahimi et al. - 2018 - HotFlip White-Box Adversarial Examples for Text C.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\9T8UQVBG\\1712.html:text/html},
}

@inproceedings{ribeiroSemanticallyEquivalentAdversarial2018,
address = {Melbourne, Australia},
title = {Semantically {Equivalent} {Adversarial} {Rules} for {Debugging} {NLP} models},
url = {https://aclanthology.org/P18-1079},
doi = {10.18653/v1/P18-1079},
abstract = {Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) – semantic-preserving perturbations that induce changes in the model's predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) – simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.},
urldate = {2021-12-12},
booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
publisher = {Association for Computational Linguistics},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
month = jul,
year = {2018},
pages = {856--865},
file = {Full Text PDF:C\:\\Users\\Admin\\Zotero\\storage\\7II4LM2S\\Ribeiro et al. - 2018 - Semantically Equivalent Adversarial Rules for Debu.pdf:application/pdf},
}

@inproceedings{pruthiLearningDeceiveAttentionBased2020a,
address = {Online},
title = {Learning to {Deceive} with {Attention}-{Based} {Explanations}},
url = {https://aclanthology.org/2020.acl-main.432},
doi = {10.18653/v1/2020.acl-main.432},
abstract = {Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention's reliability as a tool for auditing algorithms in the context of fairness and accountability.},
urldate = {2021-12-12},
booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
publisher = {Association for Computational Linguistics},
author = {Pruthi, Danish and Gupta, Mansi and Dhingra, Bhuwan and Neubig, Graham and Lipton, Zachary C.},
month = jul,
year = {2020},
pages = {4782--4793},
file = {Full Text PDF:C\:\\Users\\Admin\\Zotero\\storage\\3CUMCSMX\\Pruthi et al. - 2020 - Learning to Deceive with Attention-Based Explanati.pdf:application/pdf},
}

@article{baehrensHowExplainIndividual2009,
title = {How to {Explain} {Individual} {Classification} {Decisions}},
url = {http://arxiv.org/abs/0912.1128},
abstract = {After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted the particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.},
urldate = {2021-12-12},
journal = {arXiv:0912.1128 [cs, stat]},
author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and Mueller, Klaus-Robert},
month = dec,
year = {2009},
note = {arXiv: 0912.1128},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\QRJSWJMQ\\Baehrens et al. - 2009 - How to Explain Individual Classification Decisions.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\5GYYBLMG\\0912.html:text/html},
}

@article{ribeiroWhyShouldTrust2016,
title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
url = {http://arxiv.org/abs/1602.04938},
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
urldate = {2021-12-12},
journal = {arXiv:1602.04938 [cs, stat]},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
month = aug,
year = {2016},
note = {arXiv: 1602.04938},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\EBNAAQC8\\Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\4GUITIGW\\1602.html:text/html},
}

@article{lundbergUnifiedApproachInterpreting2017,
title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
url = {http://arxiv.org/abs/1705.07874},
abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
urldate = {2021-12-12},
journal = {arXiv:1705.07874 [cs, stat]},
author = {Lundberg, Scott and Lee, Su-In},
month = nov,
year = {2017},
note = {arXiv: 1705.07874},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\PN49CMIL\\Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\PK65BED8\\1705.html:text/html},
}

@article{slackFoolingLIMESHAP2020,
title = {Fooling {LIME} and {SHAP}: {Adversarial} {Attacks} on {Post} hoc {Explanation} {Methods}},
shorttitle = {Fooling {LIME} and {SHAP}},
url = {http://arxiv.org/abs/1911.02508},
abstract = {As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real-world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.},
urldate = {2021-12-12},
journal = {arXiv:1911.02508 [cs, stat]},
author = {Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
month = feb,
year = {2020},
note = {arXiv: 1911.02508},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\KJHEBWNW\\Slack et al. - 2020 - Fooling LIME and SHAP Adversarial Attacks on Post.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\L5RF7FGY\\1911.html:text/html},
}

@article{zhangSurveyNeuralNetwork2021,
title = {A {Survey} on {Neural} {Network} {Interpretability}},
url = {http://arxiv.org/abs/2012.14261},
abstract = {Along with the great success of deep neural networks, there is also growing concern about their black-box nature. The interpretability issue affects people's trust on deep learning systems. It is also related to many ethical problems, e.g., algorithmic discrimination. Moreover, interpretability is a desired property for deep networks to become powerful tools in other research fields, e.g., drug discovery and genomics. In this survey, we conduct a comprehensive review of the neural network interpretability research. We first clarify the definition of interpretability as it has been used in many different contexts. Then we elaborate on the importance of interpretability and propose a novel taxonomy organized along three dimensions: type of engagement (passive vs. active interpretation approaches), the type of explanation, and the focus (from local to global interpretability). This taxonomy provides a meaningful 3D view of distribution of papers from the relevant literature as two of the dimensions are not simply categorical but allow ordinal subcategories. Finally, we summarize the existing interpretability evaluation methods and suggest possible research directions inspired by our new taxonomy.},
urldate = {2021-12-12},
journal = {arXiv:2012.14261 [cs]},
author = {Zhang, Yu and Tiňo, Peter and Leonardis, Aleš and Tang, Ke},
month = jul,
year = {2021},
note = {arXiv: 2012.14261},
keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\3F5VDVVZ\\Zhang et al. - 2021 - A Survey on Neural Network Interpretability.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\TYSAGMY5\\2012.html:text/html},
}

@article{jiSurveyKnowledgeGraphs2021,
title = {A {Survey} on {Knowledge} {Graphs}: {Representation}, {Acquisition}, and {Applications}},
issn = {2162-237X, 2162-2388},
shorttitle = {A {Survey} on {Knowledge} {Graphs}},
url = {https://ieeexplore.ieee.org/document/9416312/},
doi = {10.1109/TNNLS.2021.3070843},
urldate = {2021-12-12},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
author = {Ji, Shaoxiong and Pan, Shirui and Cambria, Erik and Marttinen, Pekka and Yu, Philip S.},
year = {2021},
pages = {1--21},
file = {Submitted Version:C\:\\Users\\Admin\\Zotero\\storage\\4LW95QNW\\Ji et al. - 2021 - A Survey on Knowledge Graphs Representation, Acqu.pdf:application/pdf},
}

@article{allenInterpretingKnowledgeGraph2021,
title = {Interpreting {Knowledge} {Graph} {Relation} {Representation} from {Word} {Embeddings}},
url = {http://arxiv.org/abs/1909.11611},
abstract = {Many models learn representations of knowledge graph data by exploiting its low-rank latent structure, encoding known relations between entities and enabling unknown facts to be inferred. To predict whether a relation holds between entities, embeddings are typically compared in the latent space following a relation-specific mapping. Whilst their predictive performance has steadily improved, how such models capture the underlying latent structure of semantic information remains unexplained. Building on recent theoretical understanding of word embeddings, we categorise knowledge graph relations into three types and for each derive explicit requirements of their representations. We show that empirical properties of relation representations and the relative performance of leading knowledge graph representation methods are justified by our analysis.},
urldate = {2021-12-12},
journal = {arXiv:1909.11611 [cs, stat]},
author = {Allen, Carl and Balažević, Ivana and Hospedales, Timothy},
month = jan,
year = {2021},
note = {arXiv: 1909.11611},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\VSAW7V9D\\Allen et al. - 2021 - Interpreting Knowledge Graph Relation Representati.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\PQSEVWKT\\1909.html:text/html},
}

@inproceedings{rajaniExplainYourselfLeveraging2019,
address = {Florence, Italy},
title = {Explain {Yourself}! {Leveraging} {Language} {Models} for {Commonsense} {Reasoning}},
url = {https://aclanthology.org/P19-1487},
doi = {10.18653/v1/P19-1487},
abstract = {Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10\% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.},
urldate = {2021-12-12},
booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
publisher = {Association for Computational Linguistics},
author = {Rajani, Nazneen Fatema and McCann, Bryan and Xiong, Caiming and Socher, Richard},
month = jul,
year = {2019},
pages = {4932--4942},
file = {Full Text PDF:C\:\\Users\\Admin\\Zotero\\storage\\XHV9J26Q\\Rajani et al. - 2019 - Explain Yourself! Leveraging Language Models for C.pdf:application/pdf},
}

@article{latcinnikExplainingQuestionAnswering2020,
title = {Explaining {Question} {Answering} {Models} through {Text} {Generation}},
url = {http://arxiv.org/abs/2004.05569},
abstract = {Large pre-trained language models (LMs) have been shown to perform surprisingly well when fine-tuned on tasks that require commonsense and world knowledge. However, in end-to-end architectures, it is difficult to explain what is the knowledge in the LM that allows it to make a correct prediction. In this work, we propose a model for multi-choice question answering, where a LM-based generator generates a textual hypothesis that is later used by a classifier to answer the question. The hypothesis provides a window into the information used by the fine-tuned LM that can be inspected by humans. A key challenge in this setup is how to constrain the model to generate hypotheses that are meaningful to humans. We tackle this by (a) joint training with a simple similarity classifier that encourages meaningful hypotheses, and (b) by adding loss functions that encourage natural text without repetitions. We show on several tasks that our model reaches performance that is comparable to end-to-end architectures, while producing hypotheses that elucidate the knowledge used by the LM for answering the question.},
urldate = {2021-12-13},
journal = {arXiv:2004.05569 [cs]},
author = {Latcinnik, Veronica and Berant, Jonathan},
month = apr,
year = {2020},
note = {arXiv: 2004.05569},
keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\QFDBHT5K\\Latcinnik and Berant - 2020 - Explaining Question Answering Models through Text .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\TWNKR2TF\\2004.html:text/html},
}

@article{rossExplainingNLPModels2021,
title = {Explaining {NLP} {Models} via {Minimal} {Contrastive} {Editing} ({MiCE})},
url = {http://arxiv.org/abs/2012.13985},
abstract = {Humans have been shown to give contrastive explanations, which explain why an observed event happened rather than some other counterfactual event (the contrast case). Despite the influential role that contrastivity plays in how humans explain, this property is largely missing from current methods for explaining NLP models. We present Minimal Contrastive Editing (MiCE), a method for producing contrastive explanations of model predictions in the form of edits to inputs that change model outputs to the contrast case. Our experiments across three tasks--binary sentiment classification, topic classification, and multiple-choice question answering--show that MiCE is able to produce edits that are not only contrastive, but also minimal and fluent, consistent with human contrastive edits. We demonstrate how MiCE edits can be used for two use cases in NLP system development--debugging incorrect model outputs and uncovering dataset artifacts--and thereby illustrate that producing contrastive explanations is a promising research direction for model interpretability.},
urldate = {2021-12-13},
journal = {arXiv:2012.13985 [cs]},
author = {Ross, Alexis and Marasović, Ana and Peters, Matthew E.},
month = jun,
year = {2021},
note = {arXiv: 2012.13985},
keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\CYCNKBNB\\Ross et al. - 2021 - Explaining NLP Models via Minimal Contrastive Edit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\88J9EU2N\\2012.html:text/html},
}

@article{muCompositionalExplanationsNeurons2021,
title = {Compositional {Explanations} of {Neurons}},
url = {http://arxiv.org/abs/2006.14032},
abstract = {We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple "copy-paste" adversarial examples that change model behavior in predictable ways.},
urldate = {2021-12-13},
journal = {arXiv:2006.14032 [cs, stat]},
author = {Mu, Jesse and Andreas, Jacob},
month = feb,
year = {2021},
note = {arXiv: 2006.14032},
keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\DU3D2YB4\\Mu and Andreas - 2021 - Compositional Explanations of Neurons.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\JW896GUV\\2006.html:text/html},
}

@inproceedings{belinkovInterpretabilityAnalysisNeural2020,
address = {Online},
title = {Interpretability and {Analysis} in {Neural} {NLP}},
url = {https://aclanthology.org/2020.acl-tutorials.1},
doi = {10.18653/v1/2020.acl-tutorials.1},
abstract = {While deep learning has transformed the natural language processing (NLP) field and impacted the larger computational linguistics community, the rise of neural networks is stained by their opaque nature: It is challenging to interpret the inner workings of neural network models, and explicate their behavior. Therefore, in the last few years, an increasingly large body of work has been devoted to the analysis and interpretation of neural network models in NLP. This body of work is so far lacking a common framework and methodology. Moreover, approaching the analysis of modern neural networks can be difficult for newcomers to the field. This tutorial aims to fill this gap and introduce the nascent field of interpretability and analysis of neural networks in NLP. The tutorial will cover the main lines of analysis work, such as structural analyses using probing classifiers, behavioral studies and test suites, and interactive visualizations. We will highlight not only the most commonly applied analysis methods, but also the specific limitations and shortcomings of current approaches, in order to inform participants where to focus future efforts.},
urldate = {2021-12-13},
booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Tutorial} {Abstracts}},
publisher = {Association for Computational Linguistics},
author = {Belinkov, Yonatan and Gehrmann, Sebastian and Pavlick, Ellie},
month = jul,
year = {2020},
pages = {1--5},
file = {Full Text PDF:C\:\\Users\\Admin\\Zotero\\storage\\NCST9J3M\\Belinkov et al. - 2020 - Interpretability and Analysis in Neural NLP.pdf:application/pdf},
}

@article{belinkovAnalysisMethodsNeural2019,
title = {Analysis {Methods} in {Neural} {Language} {Processing}: {A} {Survey}},
volume = {7},
issn = {2307-387X},
shorttitle = {Analysis {Methods} in {Neural} {Language} {Processing}},
url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00254/43503/Analysis-Methods-in-Neural-Language-Processing-A},
doi = {10.1162/tacl_a_00254},
abstract = {The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their featurerich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.},
language = {en},
urldate = {2021-12-13},
journal = {Transactions of the Association for Computational Linguistics},
author = {Belinkov, Yonatan and Glass, James},
month = apr,
year = {2019},
pages = {49--72},
file = {Belinkov and Glass - 2019 - Analysis Methods in Neural Language Processing A .pdf:C\:\\Users\\Admin\\Zotero\\storage\\UNKK78IA\\Belinkov and Glass - 2019 - Analysis Methods in Neural Language Processing A .pdf:application/pdf},
}

@article{guidottiSurveyMethodsExplaining2019,
title = {A {Survey} of {Methods} for {Explaining} {Black} {Box} {Models}},
volume = {51},
issn = {0360-0300, 1557-7341},
url = {https://dl.acm.org/doi/10.1145/3236009},
doi = {10.1145/3236009},
abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
language = {en},
number = {5},
urldate = {2021-12-13},
journal = {ACM Computing Surveys},
author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
month = jan,
year = {2019},
pages = {1--42},
file = {Guidotti et al. - 2019 - A Survey of Methods for Explaining Black Box Model.pdf:C\:\\Users\\Admin\\Zotero\\storage\\3D5YHQXD\\Guidotti et al. - 2019 - A Survey of Methods for Explaining Black Box Model.pdf:application/pdf},
}

@article{guidottiSurveyMethodsExplaining2019a,
title = {A {Survey} of {Methods} for {Explaining} {Black} {Box} {Models}},
volume = {51},
issn = {0360-0300, 1557-7341},
url = {https://dl.acm.org/doi/10.1145/3236009},
doi = {10.1145/3236009},
abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
language = {en},
number = {5},
urldate = {2021-12-13},
journal = {ACM Computing Surveys},
author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
month = jan,
year = {2019},
pages = {1--42},
file = {Guidotti et al. - 2019 - A Survey of Methods for Explaining Black Box Model.pdf:C\:\\Users\\Admin\\Zotero\\storage\\BFVSYEN3\\Guidotti et al. - 2019 - A Survey of Methods for Explaining Black Box Model.pdf:application/pdf},
}

@article{shrikumarLearningImportantFeatures2019,
title = {Learning {Important} {Features} {Through} {Propagating} {Activation} {Differences}},
url = {http://arxiv.org/abs/1704.02685},
abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides: bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.},
urldate = {2021-12-13},
journal = {arXiv:1704.02685 [cs]},
author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
month = oct,
year = {2019},
note = {arXiv: 1704.02685},
keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\JP72Z75W\\Shrikumar et al. - 2019 - Learning Important Features Through Propagating Ac.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\VJAJ9LA9\\1704.html:text/html},
}

@inproceedings{hanExplainingBlackBox2020,
address = {Online},
title = {Explaining {Black} {Box} {Predictions} and {Unveiling} {Data} {Artifacts} through {Influence} {Functions}},
url = {https://aclanthology.org/2020.acl-main.492},
doi = {10.18653/v1/2020.acl-main.492},
abstract = {Modern deep learning models for NLP are notoriously opaque. This has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights. Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text. While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning. In this work, we investigate the use of influence functions for NLP, providing an alternative approach to interpreting neural text classifiers. Influence functions explain the decisions of a model by identifying influential training examples. Despite the promise of this approach, influence functions have not yet been extensively evaluated in the context of NLP, a gap addressed by this work. We conduct a comparison between influence functions and common word-saliency methods on representative tasks. As suspected, we find that influence functions are particularly useful for natural language inference, a task in which `saliency maps' may not have clear interpretation. Furthermore, we develop a new quantitative measure based on influence functions that can reveal artifacts in training data.},
urldate = {2021-12-13},
booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
publisher = {Association for Computational Linguistics},
author = {Han, Xiaochuang and Wallace, Byron C. and Tsvetkov, Yulia},
month = jul,
year = {2020},
pages = {5553--5563},
file = {Full Text PDF:C\:\\Users\\Admin\\Zotero\\storage\\WXM8Z54L\\Han et al. - 2020 - Explaining Black Box Predictions and Unveiling Dat.pdf:application/pdf},
}

@inproceedings{liVisualizingUnderstandingNeural2016,
address = {San Diego, California},
title = {Visualizing and {Understanding} {Neural} {Models} in {NLP}},
url = {https://aclanthology.org/N16-1082},
doi = {10.18653/v1/N16-1082},
urldate = {2021-12-13},
booktitle = {Proceedings of the 2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
publisher = {Association for Computational Linguistics},
author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
month = jun,
year = {2016},
pages = {681--691},
file = {Full Text PDF:C\:\\Users\\Admin\\Zotero\\storage\\2NAQVEEG\\Li et al. - 2016 - Visualizing and Understanding Neural Models in NLP.pdf:application/pdf},
}

@article{liBERTATTACKAdversarialAttack2020,
title = {{BERT}-{ATTACK}: {Adversarial} {Attack} {Against} {BERT} {Using} {BERT}},
shorttitle = {{BERT}-{ATTACK}},
url = {http://arxiv.org/abs/2004.09984},
abstract = {Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose {\textbackslash}textbf\{BERT-Attack\}, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at https://github.com/LinyangLee/BERT-Attack.},
urldate = {2021-12-13},
journal = {arXiv:2004.09984 [cs]},
author = {Li, Linyang and Ma, Ruotian and Guo, Qipeng and Xue, Xiangyang and Qiu, Xipeng},
month = oct,
year = {2020},
note = {arXiv: 2004.09984},
keywords = {Computer Science - Computation and Language},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\YU7REYG9\\Li et al. - 2020 - BERT-ATTACK Adversarial Attack Against BERT Using.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\2D4GC2M3\\2004.html:text/html},
}

@article{sahaUsingTsetlinMachine,
title = {Using {Tsetlin} {Machine} to discover interpretable rules in natural language processing applications},
volume = {n/a},
issn = {1468-0394},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12873},
doi = {10.1111/exsy.12873},
abstract = {Tsetlin Machines (TM) use finite state machines for learning and propositional logic to represent patterns. The resulting pattern recognition approach captures information in the form of conjunctive clauses, thus facilitating human interpretation. In this work, we propose a TM-based approach to three common natural language processing (NLP) tasks, namely, sentiment analysis, semantic relation categorization and identifying entities in multi-turn dialogues. By performing frequent itemset mining on the TM-produced patterns, we show that we can obtain a global and a local interpretation of the learning, one that mimics existing rule-sets or lexicons. Further, we also establish that our TM based approach does not compromise on accuracy in the quest for interpretability, via comparison with some widely used machine learning techniques. Finally, we introduce the idea of a relational TM, which uses a logic-based framework to further extend the interpretability.},
language = {en},
number = {n/a},
urldate = {2021-12-13},
journal = {Expert Systems},
author = {Saha, Rupsa and Granmo, Ole-Christoffer and Goodwin, Morten},
note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/exsy.12873},
keywords = {artificial intelligence, interpretable AI, multi-turn dialogue analysis, natural language processing, rule mining, semantic analysis, sentiment analysis},
pages = {e12873},
file = {Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\4554R3UT\\exsy.html:text/html;Full Text PDF:C\:\\Users\\Admin\\Zotero\\storage\\DPE4YXYQ\\Saha et al. - Using Tsetlin Machine to discover interpretable ru.pdf:application/pdf},
}
