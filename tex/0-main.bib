
@article{madsen_post-hoc_2021,
title = {Post-hoc {Interpretability} for {Neural} {NLP}: {A} {Survey}},
shorttitle = {Post-hoc {Interpretability} for {Neural} {NLP}},
url = {http://arxiv.org/abs/2108.04840},
abstract = {Natural Language Processing (NLP) models have become increasingly more complex and widespread. With recent developments in neural networks, a growing concern is whether it is responsible to use these models. Concerns such as safety and ethics can be partially addressed by providing explanations. Furthermore, when models do fail, providing explanations is paramount for accountability purposes. To this end, interpretability serves to provide these explanations in terms that are understandable to humans. Central to what is understandable is how explanations are communicated. Therefore, this survey provides a categorization of how recent interpretability methods communicate explanations and discusses the methods in depth. Furthermore, the survey focuses on post-hoc methods, which provide explanations after a model is learned and generally model-agnostic. A common concern for this class of methods is whether they accurately reflect the model. Hence, how these post-hoc methods are evaluated is discussed throughout the paper.},
urldate = {2021-10-28},
journal = {arXiv:2108.04840 [cs]},
author = {Madsen, Andreas and Reddy, Siva and Chandar, Sarath},
month = aug,
year = {2021},
note = {arXiv: 2108.04840},
keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
file = {arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\CMGIH57X\\2108.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\VFUJHGWH\\Madsen и др. - 2021 - Post-hoc Interpretability for Neural NLP A Survey.pdf:application/pdf},
}

@article{alshargi_concept2vec_2020,
title = {Concept2vec: {Metrics} for {Evaluating} {Quality} of {Embeddings} for {Ontological} {Concepts}},
shorttitle = {Concept2vec},
url = {http://arxiv.org/abs/1803.04488},
abstract = {Although there is an emerging trend towards generating embeddings for primarily unstructured data and, recently, for structured data, no systematic suite for measuring the quality of embeddings has been proposed yet. This deficiency is further sensed with respect to embeddings generated for structured data because there are no concrete evaluation metrics measuring the quality of the encoded structure as well as semantic patterns in the embedding space. In this paper, we introduce a framework containing three distinct tasks concerned with the individual aspects of ontological concepts: (i) the categorization aspect, (ii) the hierarchical aspect, and (iii) the relational aspect. Then, in the scope of each task, a number of intrinsic metrics are proposed for evaluating the quality of the embeddings. Furthermore, w.r.t. this framework, multiple experimental studies were run to compare the quality of the available embedding models. Employing this framework in future research can reduce misjudgment and provide greater insight about quality comparisons of embeddings for ontological concepts. We positioned our sampled data and code at https://github.com/alshargi/Concept2vec under GNU General Public License v3.0.},
urldate = {2021-11-14},
journal = {arXiv:1803.04488 [cs]},
author = {Alshargi, Faisal and Shekarpour, Saeedeh and Soru, Tommaso and Sheth, Amit},
month = may,
year = {2020},
note = {arXiv: 1803.04488},
keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, I.2.6, I.2.4, Concept2Vec},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\II7AMTJU\\Alshargi и др. - 2020 - Concept2vec Metrics for Evaluating Quality of Emb.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\WNB2KHDQ\\1803.html:text/html;concept2vec.md:C\:\\Users\\Admin\\Dropbox\\Vaults\\interpretable-nlp\\Interpretable NLP\\Concept2Vec\\concept2vec.md:text/plain},
}

@article{alatrish_building_2014,
title = {Building ontologies for different natural languages},
volume = {11},
issn = {1820-0214, 2406-1018},
url = {http://www.doiserbia.nb.rs/Article.aspx?ID=1820-02141400023A},
doi = {10.2298/CSIS130429023A},
abstract = {Ontology construction of a certain domain is an important step in applying the Semantic web. A number of software tools adapted for building domain ontologies of most wide–spread natural languages are available, but accomplishing that for any given natural language presents a challenge. Here we propose a semi-automatic procedure to create ontologies for different natural languages. Our approach utilizes various software tools available on the Internet most notably DODDLE-OWL - a domain ontology development tool implemented for English and Japanese languages. By using this tool, WordNet, Protégé and XSLT transformations, we propose a general procedure to construct domain ontology for any natural language.},
language = {en},
number = {2},
urldate = {2021-11-14},
journal = {Computer Science and Information Systems},
author = {Alatrish, Emhimed and Tosic, Dusan and Milenkovic, Nikola},
year = {2014},
pages = {623--644},
file = {Alatrish и др. - 2014 - Building ontologies for different natural language.pdf:C\:\\Users\\Admin\\Zotero\\storage\\WV39J93V\\Alatrish и др. - 2014 - Building ontologies for different natural language.pdf:application/pdf},
}

@incollection{hutchison_dbpedia_2007,
address = {Berlin, Heidelberg},
title = {{DBpedia}: {A} {Nucleus} for a {Web} of {Open} {Data}},
volume = {4825},
isbn = {978-3-540-76297-3 978-3-540-76298-0},
shorttitle = {{DBpedia}},
url = {http://link.springer.com/10.1007/978-3-540-76298-0_52},
abstract = {DBpedia is a community eﬀort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human- and machine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.},
language = {en},
urldate = {2021-11-14},
booktitle = {The {Semantic} {Web}},
publisher = {Springer Berlin Heidelberg},
author = {Auer, Sören and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Aberer, Karl and Choi, Key-Sun and Noy, Natasha and Allemang, Dean and Lee, Kyung-Il and Nixon, Lyndon and Golbeck, Jennifer and Mika, Peter and Maynard, Diana and Mizoguchi, Riichiro and Schreiber, Guus and Cudré-Mauroux, Philippe},
year = {2007},
doi = {10.1007/978-3-540-76298-0_52},
note = {Series Title: Lecture Notes in Computer Science},
pages = {722--735},
file = {Auer и др. - 2007 - DBpedia A Nucleus for a Web of Open Data.pdf:C\:\\Users\\Admin\\Zotero\\storage\\MHCXRR68\\Auer и др. - 2007 - DBpedia A Nucleus for a Web of Open Data.pdf:application/pdf;DBpedia-article.md:C\:\\Users\\Admin\\Dropbox\\Vaults\\interpretable-nlp\\Interpretable NLP\\Concept2Vec\\DBpedia-article.md:text/plain},
}

@article{doshi-velez_towards_2017,
title = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
url = {http://arxiv.org/abs/1702.08608},
abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
urldate = {2021-11-17},
journal = {arXiv:1702.08608 [cs, stat]},
author = {Doshi-Velez, Finale and Kim, Been},
month = mar,
year = {2017},
note = {arXiv: 1702.08608},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\Q3SSDAR9\\Doshi-Velez и Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\QH9HISC7\\1702.html:text/html;Towards a Rigorous Science of Interpretable Machine Learning:C\:\\Users\\Admin\\Dropbox\\Vaults\\interpretable-nlp\\Papers To Read\\Towards a Rigorous Science of Interpretable Machine Learning.md:text/plain},
}

@article{lipton_mythos_2018,
title = {The mythos of model interpretability},
volume = {61},
issn = {0001-0782},
url = {https://doi.org/10.1145/3233231},
doi = {10.1145/3233231},
abstract = {In machine learning, the concept of interpretability is both important and slippery.},
number = {10},
urldate = {2021-11-17},
journal = {Communications of the ACM},
author = {Lipton, Zachary C.},
month = sep,
year = {2018},
pages = {36--43},
file = {Отправленная версия:C\:\\Users\\Admin\\Zotero\\storage\\XLESK2PE\\Lipton - 2018 - The mythos of model interpretability.pdf:application/pdf;Lipton 2018.md:C\:\\Users\\Admin\\Dropbox\\Vaults\\interpretable-nlp\\Papers\\Papers To Read\\Lipton 2018.md:text/plain},
}

@article{lai_ontology-based_2020,
title = {Ontology-based {Interpretable} {Machine} {Learning} for {Textual} {Data}},
url = {http://arxiv.org/abs/2004.00204},
abstract = {In this paper, we introduce a novel interpreting framework that learns an interpretable model based on an ontology-based sampling technique to explain agnostic prediction models. Different from existing approaches, our algorithm considers contextual correlation among words, described in domain knowledge ontologies, to generate semantic explanations. To narrow down the search space for explanations, which is a major problem of long and complicated text data, we design a learnable anchor algorithm, to better extract explanations locally. A set of regulations is further introduced, regarding combining learned interpretable representations with anchors to generate comprehensible semantic explanations. An extensive experiment conducted on two real-world datasets shows that our approach generates more precise and insightful explanations compared with baseline approaches.},
urldate = {2021-11-17},
journal = {arXiv:2004.00204 [cs, stat]},
author = {Lai, Phung and Phan, NhatHai and Hu, Han and Badeti, Anuja and Newman, David and Dou, Dejing},
month = mar,
year = {2020},
note = {arXiv: 2004.00204},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\7ZLRVKV6\\Lai и др. - 2020 - Ontology-based Interpretable Machine Learning for .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\LX49TDIE\\2004.html:text/html},
}

@article{cheng_improving_2020,
title = {Improving {Disentangled} {Text} {Representation} {Learning} with {Information}-{Theoretic} {Guidance}},
url = {http://arxiv.org/abs/2006.00693},
abstract = {Learning disentangled representations of natural language is essential for many NLP tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc. Similar problems have been studied extensively for other forms of data, such as images and videos. However, the discrete nature of natural language makes the disentangling of textual representations more challenging (e.g., the manipulation over the data space cannot be easily achieved). Inspired by information theory, we propose a novel method that effectively manifests disentangled representations of text, without any supervision on semantics. A new mutual information upper bound is derived and leveraged to measure dependence between style and content. By minimizing this upper bound, the proposed method induces style and content embeddings into two independent low-dimensional spaces. Experiments on both conditional text generation and text-style transfer demonstrate the high quality of our disentangled representation in terms of content and style preservation.},
urldate = {2021-11-17},
journal = {arXiv:2006.00693 [cs, stat]},
author = {Cheng, Pengyu and Min, Martin Renqiang and Shen, Dinghan and Malon, Christopher and Zhang, Yizhe and Li, Yitong and Carin, Lawrence},
month = jun,
year = {2020},
note = {arXiv: 2006.00693},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\4PWUQYIE\\Cheng и др. - 2020 - Improving Disentangled Text Representation Learnin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\4WANT5X8\\2006.html:text/html;Cheng (2020).md:C\:\\Users\\Admin\\Dropbox\\Vaults\\interpretable-nlp\\Papers\\Papers To Read\\Cheng (2020).md:text/plain},
}

@article{zhang_disentangling_2021,
title = {Disentangling {Representations} of {Text} by {Masking} {Transformers}},
url = {http://arxiv.org/abs/2104.07155},
abstract = {Representations from large pretrained models such as BERT encode a range of features into monolithic vectors, affording strong predictive accuracy across a multitude of downstream tasks. In this paper we explore whether it is possible to learn disentangled representations by identifying existing subnetworks within pretrained models that encode distinct, complementary aspect representations. Concretely, we learn binary masks over transformer weights or hidden units to uncover subsets of features that correlate with a specific factor of variation; this eliminates the need to train a disentangled model from scratch for a particular task. We evaluate this method with respect to its ability to disentangle representations of sentiment from genre in movie reviews, "toxicity" from dialect in Tweets, and syntax from semantics. By combining masking with magnitude pruning we find that we can identify sparse subnetworks within BERT that strongly encode particular aspects (e.g., toxicity) while only weakly encoding others (e.g., race). Moreover, despite only learning masks, we find that disentanglement-via-masking performs as well as -- and often better than -- previously proposed methods based on variational autoencoders and adversarial training.},
urldate = {2021-11-17},
journal = {arXiv:2104.07155 [cs]},
author = {Zhang, Xiongyi and van de Meent, Jan-Willem and Wallace, Byron C.},
month = sep,
year = {2021},
note = {arXiv: 2104.07155},
keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\ZT92TSE6\\Zhang и др. - 2021 - Disentangling Representations of Text by Masking T.pdf:application/pdf;Zhang (2021).md:C\:\\Users\\Admin\\Dropbox\\Vaults\\interpretable-nlp\\Papers\\Papers To Read\\Zhang (2021).md:text/plain;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\H9YJUHWS\\2104.html:text/html},
}

@article{bolukbasi_interpretability_2021,
title = {An {Interpretability} {Illusion} for {BERT}},
url = {http://arxiv.org/abs/2104.07143},
abstract = {We describe an "interpretability illusion" that arises when analyzing the BERT model. Activations of individual neurons in the network may spuriously appear to encode a single, simple concept, when in fact they are encoding something far more complex. The same effect holds for linear combinations of activations. We trace the source of this illusion to geometric properties of BERT's embedding space as well as the fact that common text corpora represent only narrow slices of possible English sentences. We provide a taxonomy of model-learned concepts and discuss methodological implications for interpretability research, especially the importance of testing hypotheses on multiple data sets.},
urldate = {2021-11-23},
journal = {arXiv:2104.07143 [cs]},
author = {Bolukbasi, Tolga and Pearce, Adam and Yuan, Ann and Coenen, Andy and Reif, Emily and Viégas, Fernanda and Wattenberg, Martin},
month = apr,
year = {2021},
note = {arXiv: 2104.07143},
keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\EAGJVUTX\\Bolukbasi и др. - 2021 - An Interpretability Illusion for BERT.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\QNUUGSDT\\2104.html:text/html},
}

@article{shekarpour_cevo_2018,
title = {{CEVO}: {Comprehensive} {EVent} {Ontology} {Enhancing} {Cognitive} {Annotation}},
shorttitle = {{CEVO}},
url = {http://arxiv.org/abs/1701.05625},
abstract = {While the general analysis of named entities has received substantial research attention on unstructured as well as structured data, the analysis of relations among named entities has received limited focus. In fact, a review of the literature revealed a deficiency in research on the abstract conceptualization required to organize relations. We believe that such an abstract conceptualization can benefit various communities and applications such as natural language processing, information extraction, machine learning, and ontology engineering. In this paper, we present Comprehensive EVent Ontology (CEVO), built on Levin's conceptual hierarchy of English verbs that categorizes verbs with shared meaning, and syntactic behavior. We present the fundamental concepts and requirements for this ontology. Furthermore, we present three use cases employing the CEVO ontology on annotation tasks: (i) annotating relations in plain text, (ii) annotating ontological properties, and (iii) linking textual relations to ontological properties. These use-cases demonstrate the benefits of using CEVO for annotation: (i) annotating English verbs from an abstract conceptualization, (ii) playing the role of an upper ontology for organizing ontological properties, and (iii) facilitating the annotation of text relations using any underlying vocabulary. This resource is available at https://shekarpour.github.io/cevo.io/ using https://w3id.org/cevo namespace.},
urldate = {2021-11-23},
journal = {arXiv:1701.05625 [cs]},
author = {Shekarpour, Saeedeh and Alshargi, Faisal and Shalin, Valerie and Thirunarayan, Krishnaprasad and Sheth, Amit P.},
month = oct,
year = {2018},
note = {arXiv: 1701.05625
version: 2},
keywords = {Computer Science - Computation and Language},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\YHMVN7W4\\Shekarpour и др. - 2018 - CEVO Comprehensive EVent Ontology Enhancing Cogni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\GBV2T2UG\\1701.html:text/html},
}

@article{paccanaro_learning_2000,
title = {Learning distributed representations of concepts from relational data using linear relational embedd},
journal = {IEEE Transactions on Knowledge and Data Engineering - TKDE},
author = {Paccanaro, Alberto and Hinton, G.},
month = jan,
year = {2000},
}

@misc{he_interpretable_2021,
title = {Interpretable {NLP}},
copyright = {MIT},
url = {https://github.com/ShilinHe/interpretableNLP},
abstract = {A list of publications on NLP interpretability (Welcome PR)},
urldate = {2021-11-23},
author = {HE, Shilin},
month = nov,
year = {2021},
note = {original-date: 2019-07-11T01:35:34Z},
}

@article{pruthi_learning_2020,
title = {Learning to {Deceive} with {Attention}-{Based} {Explanations}},
url = {http://arxiv.org/abs/1909.07913},
abstract = {Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention's reliability as a tool for auditing algorithms in the context of fairness and accountability.},
urldate = {2021-11-23},
journal = {arXiv:1909.07913 [cs]},
author = {Pruthi, Danish and Gupta, Mansi and Dhingra, Bhuwan and Neubig, Graham and Lipton, Zachary C.},
month = apr,
year = {2020},
note = {arXiv: 1909.07913},
keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\EUQSJKLS\\Pruthi и др. - 2020 - Learning to Deceive with Attention-Based Explanati.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\WYHUE8EZ\\1909.html:text/html},
}

@article{he_deberta_2021,
title = {{DeBERTa}: {Decoding}-enhanced {BERT} with {Disentangled} {Attention}},
shorttitle = {{DeBERTa}},
url = {http://arxiv.org/abs/2006.03654},
abstract = {Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9\% (90.2\% vs. 91.1\%), on SQuAD v2.0 by +2.3\% (88.4\% vs. 90.7\%) and RACE by +3.6\% (83.2\% vs. 86.8\%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).},
urldate = {2021-11-24},
journal = {arXiv:2006.03654 [cs]},
author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
month = oct,
year = {2021},
note = {arXiv: 2006.03654},
keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, cs.CL, cs.GL, I.2, I.7},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\MYURRFX4\\He и др. - 2021 - DeBERTa Decoding-enhanced BERT with Disentangled .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\L7MQBA32\\2006.html:text/html;undefined-zotero.md:C\:\\Users\\Admin\\Dropbox\\Vaults\\interpretable-nlp\\Papers\\undefined-zotero.md:text/plain},
}

@article{wu_polyjuice_2021,
title = {Polyjuice: {Generating} {Counterfactuals} for {Explaining}, {Evaluating}, and {Improving} {Models}},
shorttitle = {Polyjuice},
url = {http://arxiv.org/abs/2101.00288},
abstract = {While counterfactual examples are useful for analysis and training of NLP models, current generation methods either rely on manual labor to create very few counterfactuals, or only instantiate limited types of perturbations such as paraphrases or word substitutions. We present Polyjuice, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning GPT-2 on multiple datasets of paired sentences. We show that Polyjuice produces diverse sets of realistic counterfactuals, which in turn are useful in various distinct applications: improving training and evaluation on three different tasks (with around 70\% less annotation effort than manual generation), augmenting state-of-the-art explanation techniques, and supporting systematic counterfactual error analysis by revealing behaviors easily missed by human experts.},
urldate = {2021-11-26},
journal = {arXiv:2101.00288 [cs]},
author = {Wu, Tongshuang and Ribeiro, Marco Tulio and Heer, Jeffrey and Weld, Daniel S.},
month = jun,
year = {2021},
note = {arXiv: 2101.00288},
keywords = {Computer Science - Computation and Language},
file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\AYKAVFJ8\\Wu и др. - 2021 - Polyjuice Generating Counterfactuals for Explaini.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\393SEYRB\\2101.html:text/html;undefined-zotero.md:C\:\\Users\\Admin\\Dropbox\\Vaults\\interpretable-nlp\\Papers\\__Papers To Read\\undefined-zotero.md:text/plain},
}

@book{molnar_interpretable_nodate,
title = {Interpretable {Machine} {Learning}},
url = {https://christophm.github.io/interpretable-ml-book/index.html},
abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
urldate = {2021-12-11},
author = {Molnar, Christoph},
file = {Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\R79DF8TW\\index.html:text/html},
}
